{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+9ChiBlJrdODMNQoltePk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhamjain2409/Context_Retriever_System/blob/main/Context_Retriever_Sentence_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6adDRgftceU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#DOne\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document as DocxDocument\n",
        "from langchain.schema import Document\n",
        "import re\n",
        "\n",
        "def read_docx_to_text(docx_path):\n",
        "    doc = DocxDocument(docx_path)\n",
        "    full_text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
        "    return full_text\n",
        "\n",
        "# Simple regex-based sentence tokenizer\n",
        "def regex_sent_tokenize(text):\n",
        "    return re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "def split_by_section_and_sentences(text, sentences_per_chunk=5):\n",
        "    sections = re.split(r'(?=\\n\\d+\\.\\s)', text)\n",
        "    chunks = []\n",
        "\n",
        "    for section in sections:\n",
        "        lines = section.strip().split('\\n', 1)\n",
        "        title = lines[0].strip()\n",
        "        content = lines[1].strip() if len(lines) > 1 else \"\"\n",
        "\n",
        "        sentences = regex_sent_tokenize(content)\n",
        "        for i in range(0, len(sentences), sentences_per_chunk):\n",
        "            chunk_text = \" \".join(sentences[i:i + sentences_per_chunk])\n",
        "            chunks.append(Document(page_content=f\"{title}\\n{chunk_text}\", metadata={\"section\": title}))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# === USAGE ===\n",
        "docx_file_path = \"/content/drive/My Drive/LLMda_Data/Applied_AI_Engineer/Sample_Sources/USB3_IP_User_Guide_v2.docx\"\n",
        "text = read_docx_to_text(docx_file_path)\n",
        "chunks = split_by_section_and_sentences(text, sentences_per_chunk=5)"
      ],
      "metadata": {
        "id": "sMeQ893dtsRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_retriever(chunks):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings\n",
        "    from langchain.schema import Document\n",
        "    import uuid\n",
        "\n",
        "    def query_preprocess(query: str) -> str:\n",
        "        return f\"query: {query}\"\n",
        "\n",
        "    def passage_preprocess(doc: Document) -> Document:\n",
        "        doc.page_content = f\"passage: {doc.page_content}\"\n",
        "        return doc\n",
        "\n",
        "    chunks = [passage_preprocess(chunk) for chunk in chunks]\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"intfloat/e5-large-v2\",\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "    # Create a unique, fresh Chroma collection (no persist_directory = in-memory)\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=f\"e5_collection_{uuid.uuid4()}\"  # ensures a fresh collection\n",
        "    )\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "    class E5Retriever:\n",
        "        def __init__(self, retriever):\n",
        "            self.retriever = retriever\n",
        "\n",
        "        def get_relevant_documents(self, query):\n",
        "            return self.retriever.get_relevant_documents(query_preprocess(query))\n",
        "\n",
        "    return E5Retriever(retriever)\n"
      ],
      "metadata": {
        "id": "jZzPSEVQtj6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = create_embedding_retriever(chunks)"
      ],
      "metadata": {
        "id": "fchrTYqttj8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_relevant_paragraphs(query, retriever, k):\n",
        "\n",
        "    return retriever.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "cMXHziKvtj_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How does the USB 3.0 IP core handle link error recovery?\"\n",
        "top_docs = get_top_k_relevant_paragraphs(query, retriever, \"PCIe Gen 5 x8 IP core\", 3)\n"
      ],
      "metadata": {
        "id": "A59q70t3tkBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = [doc.page_content.strip() for doc in top_docs]\n",
        "\n",
        "for i, para in enumerate(paragraphs, 1):\n",
        "    print(f\"Paragraph {i}:\\n{para}\\n\")"
      ],
      "metadata": {
        "id": "ojXy9Su8tkEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5l5YI4utkGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UxKPtnZntkJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}